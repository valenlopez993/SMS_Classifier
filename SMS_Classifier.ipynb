{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valenlopez993/SMS_Classifier/blob/main/SMS_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqwzlwLeI_VM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S_oeOX0EJFF2"
      },
      "source": [
        "# Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !wget https://raw.githubusercontent.com/valenlopez993/SMS_Classifier/main/train-data.tsv\n",
        "    !wget https://raw.githubusercontent.com/valenlopez993/SMS_Classifier/main/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly, two data frames are created: one containing the training data and the other containing the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1fRway5oZDl"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(train_file_path, sep='\\t', header=None, names=['ham_spam', 'mail'])\n",
        "df_test = pd.read_csv(test_file_path, sep='\\t', header=None, names=['ham_spam', 'mail'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this project are two classifications for the emails: **ham** or **spam**. So first the labels have to be separated from the data and then converted to a numerical value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Irg4ZgwsHX0"
      },
      "outputs": [],
      "source": [
        "df_train_labels = df_train.pop('ham_spam')\n",
        "df_test_labels = df_test.pop('ham_spam')\n",
        "\n",
        "df_train_labels = pd.Categorical(df_train_labels)\n",
        "df_test_labels = pd.Categorical(df_test_labels)\n",
        "\n",
        "train_labels = df_train_labels.codes\n",
        "test_labels = df_test_labels.codes\n",
        "\n",
        "categories = df_train_labels.unique()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It isn't possible to pass strings to the model so each word has to be encoded. There are several ways to do that but here it's used the `Tokenizer` class from Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBWLgutJJTh0"
      },
      "outputs": [],
      "source": [
        "train_to_tokenize = df_train.to_numpy().reshape(df_train.shape[0])\n",
        "train_tokenizer = Tokenizer()\n",
        "train_tokenizer.fit_on_texts(train_to_tokenize)\n",
        "train_tokenized = train_tokenizer.texts_to_sequences(train_to_tokenize)\n",
        "\n",
        "test_to_tokenize = df_test.to_numpy().reshape(df_test.shape[0])\n",
        "test_tokenizer = Tokenizer()\n",
        "test_tokenizer.fit_on_texts(test_to_tokenize)\n",
        "test_tokenized = test_tokenizer.texts_to_sequences(test_to_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REOVq0bmJ2DX"
      },
      "outputs": [],
      "source": [
        "word_index = train_tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another important thing to take into account it's the size of each input data. It's mandatory to feed the model with data that has the same size. But this is not always the case when it's talked about words and phrases so here it's defined a `maxLength`:\n",
        "\n",
        "- if the email is greater than 255 words then trim off the extra words\n",
        "- if the email is less than 255 words add the necessary amount of 0's to make it equal to 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "maxLength = 255\n",
        "train_data = pad_sequences(train_tokenized, maxLength)\n",
        "test_data = pad_sequences(test_tokenized, maxLength)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pX69jPnjgGQ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AmTykStku7i"
      },
      "outputs": [],
      "source": [
        "model.fit(train_data, train_labels, epochs=50)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "anbKD68Njc_x"
      },
      "source": [
        "# Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [],
      "source": [
        "def predict_message(pred_text):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts([pred_text])\n",
        "  tokenized = train_tokenizer.texts_to_sequences([pred_text])\n",
        "  text_to_predict = pad_sequences(tokenized, maxLength)\n",
        "\n",
        "  model_prediction = model.predict(text_to_predict, verbose=0)\n",
        "  \n",
        "  prediction = []\n",
        "  prediction.append(model_prediction[0][0])\n",
        "  prediction.append(categories[int(np.round(model_prediction)[0])])\n",
        "\n",
        "  print(pred_text, \"===>\", prediction)\n",
        "\n",
        "  return prediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [],
      "source": [
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won Â£1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "\n",
        "  for msg, ans in zip(test_messages, answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      print(\"WRONG\\n\")\n",
        "    else:\n",
        "      print(\"CORRECT!!!\\n\")\n",
        "\n",
        "test_predictions()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
